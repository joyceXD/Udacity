BankData <- read.csv("C:/Users/310195644/Desktop/ds interview/Prob_BankData/BankData.csv")
View(BankData)
summary(BankData)
install.packages("boruta")
install.packages("Boruta")
y.4 <- factor(rowSums(x[, 1:n.dep] >= 0) %% 2)
b.4 <- Boruta(x, y.4, doTrace = 2)
print(b.4)
make.plots(b.4, 4, n.dep)
x <- data.frame(V=matrix(rnorm(n.var*n.obs), n.obs, n.var))
run.name <- "feature-1"
library("Boruta")
set.seed(1)
## Set up artificial test data for our analysis
n.var <- 20
n.obs <- 200
x <- data.frame(V=matrix(rnorm(n.var*n.obs), n.obs, n.var))
y.4 <- factor(rowSums(x[, 1:n.dep] >= 0) %% 2)
b.4 <- Boruta(x, y.4, doTrace = 2)
print(b.4)
make.plots(b.4, 4, n.dep)
run.name <- "feature-1"
library("Boruta")
set.seed(1)
## Set up artificial test data for our analysis
n.var <- 20
n.obs <- 200
n.dep <- floor(n.var/5)
x <- data.frame(V=matrix(rnorm(n.var*n.obs), n.obs, n.var))
y.4 <- factor(rowSums(x[, 1:n.dep] >= 0) %% 2)
b.4 <- Boruta(x, y.4, doTrace = 2)
print(b.4)
make.plots(b.4, 4, n.dep)
make.plots(b.4, 4, n.dep)
## Utility function to make plots of Boruta test results
make.plots <- function(b, num,
true.var = NA,
main = paste("Boruta feature selection for test", num)) {
write.text <- function(b, true.var) {
if ( !is.na(true.var) ) {
text(1, max(attStats(b)$meanZ), pos = 4,
labels = paste("True vars are V.1-V.",
true.var, sep = ""))
}
}
plot(b, main = main, las = 3, xlab = "")
write.text(b, true.var)
png(paste(run.name, num, "png", sep = "."), width = 8, height = 8,
units = "cm", res = 300, pointsize = 4)
plot(b, main = main, lwd = 0.5, las = 3, xlab = "")
write.text(b, true.var)
dev.off()
}
make.plots(b.4, 4, n.dep)
gsub('*[UserInfo=*][{]*[}]*', "", "Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ")
gsub('*[UserInfo=*][{]*[}]*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*UserInfo=*[{]*[}]*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{]*[}]*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{]{1}[}]{1}*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{][}]*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{]+[}]+*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{*}]*', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
gsub('*[{*}]+', "", 'Error Domain=CBErrorDomain Code=7 "The specified device has disconnected from us." UserInfo=0x1a743d00 {NSLocalizedDescription=The specified device has disconnected from us.} ')
View(BankData)
aggregate(. ~ age + job,)
aggregate(. ~ age + job, BankData, FUN = head)
x <- aggregate(. ~ age + job, BankData, FUN = head)
View(x)
missing.file.list_1 <- read.delim("C:/Users/310195644/Desktop/missing file list_1.txt", header=FALSE, stringsAsFactors=FALSE)
View(missing.file.list_1)
missing.file <- as.vector(as.matrix(missing.file.list_1))
getdate <- function(x){
return(gsub('*_*.tsv.gz','\\1', x))
}
missing.file.1 <- apply(missing.file, getdate)
missing.file.1 <- apply(missing.file, getdate)
getdate <- function(x){
return(gsub('*_*.tsv.gz','\\1', x))
}
missing.file.1 <- apply(missing.file, getdate)
missing.file.1 <- apply(missing.file, function(x) return(gsub('*_*.tsv.gz','\\1', x)))
missing.file.1 <- apply(missing.file, 1, function(x) return(gsub('*_*.tsv.gz','\\1', x)))
missing.file.1 <- apply(missing.file, 2, function(x) return(gsub('*_*.tsv.gz','\\1', x)))
missing.file.1 <- lapply(missing.file, function(x) return(gsub('*_*.tsv.gz','\\1', x)))
missing.file.1 <- lapply(missing.file, function(x) return(gsub('*-*_[1-9]*.tsv.gz','\\1', x)))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*$",'', x)))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*.tsv.gz",'', x)))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*",'', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(gsub("*.tsv.gz", '', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(strptime(x, "%Y%m%d-%H%M%S")))
missing.file.1 <- as.data.frame(missing.file.1)
View(missing.file.1)
missing.file.1 <- as.data.frame(c(missing.file.1))
missing.file <- as.vector(as.matrix(missing.file.list_1))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*",'', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(gsub("*.tsv.gz", '', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(strptime(x, "%Y%m%d-%H%M%S")))
View(missing.file.list_1)
missing.file.2 <- lapply(missing.file.1, function(x) return(as.Date(x)))
library("lubridate", lib.loc="C:/Program Files/R/R-3.2.3/library")
missing.file.1 <- lapply(missing.file.1, function(x) return(ymd_hms(x)))
missing.file <- as.vector(as.matrix(missing.file.list_1))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*",'', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(gsub("*.tsv.gz", '', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(strptime(x, "%Y%m%d-%H%M%S")))
missing.file.2 <- lapply(missing.file.1, function(x) return(as.Date(x)))
missing.dates <- as.data.frame(table(missing.file.2))
table(missing.file.2)
missing.dates <- as.data.frame(summary(missing.file.2))
View(missing.dates)
write.csv(missing.file.2, "missingdates.csv")
getwd
getwd()
write.table(data.frame(missing.dates), "date.xls", col.names = TRUE, row.names =
FALSE)
write.table(data.frame(missing.file.2), "date.xls", col.names = TRUE, row.names =
FALSE)
missing.file.2 <- c(missing.file.2)
missing.file.2 <- as.vector(missing.file.2)
write.table(data.frame(missing.file.2), "date.xls", col.names = TRUE, row.names =
FALSE)
View(missing.dates)
rep(c(1,2, 2, 2), 25)
table(missing.file.2)
missing.file.list_1 <- read.delim("C:/Users/310195644/Desktop/missing file list_1.txt", header=FALSE)
View(missing.file.list_1)
missing.file <- as.vector(as.matrix(missing.file.list_1))
missing.file.1 <- lapply(missing.file, function(x) return(gsub("^[^_]*_|_[^_]*",'', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(gsub("*.tsv.gz", '', x)))
missing.file.1 <- lapply(missing.file.1, function(x) return(strptime(x, "%Y%m%d-%H%M%S")))
missing.file.2 <- lapply(missing.file.1, function(x) return(as.Date(x)))
table(missing.file.2)
missing.file.3 <- lapply(missing.file.1, function(x) return(hour(x)))
missing.file.4 <- cbind(missing.file.2, missing.file.3)
missing.file.4 <- rbind(missing.file.2, missing.file.3)
missing.file.4 <- cbind.data.frame(missing.file.2, missing.file.3)
missing.file.4 <- cbind(missing.file.2, missing.file.3)
missing.file.4 <- data.frame(date = missing.file.2, hour = missing.file.3)
require(reshape2)
df <- melt(data.frame(missing.file.2, missing.file.3))
colnames(df) <- c("date", "hour")
View(df)
df <- melt(data.frame(as.character(missing.file.2), missing.file.3))
colnames(df) <- c("date", "hour")
View(df)
missing.file.2 <- lapply(missing.file.1, function(x) return(as.charactor(as.Date(x))))
rm(df)
missing.file.2 <- lapply(missing.file.1, function(x) return(as.character(as.Date(x))))
missing.file.3 <- lapply(missing.file.1, function(x) return(as.integer(hour(x))))
library(lubridate)
missing.file.3 <- lapply(missing.file.1, function(x) return(as.integer(hour(x))))
require(reshape2)
df <- melt(data.frame(missing.file.2, missing.file.3))
colnames(df) <- c("date", "hour")
View(df)
df <- melt(data.frame(cbind(missing.file.2, missing.file.3)))
df <- data.frame(cbind(missing.file.2, missing.file.3))
View(df)
df.missing <- aggregate(. ~ date,
data = df,
FUN = length)
View(df)
df.missing <- aggregate(. ~ missing.file.2,
data = df,
FUN = length)
View(df)
write.csv(df, "missing_date.csv")
View(df)
write.table(df, "missing_date.csv")
View(missing.dates)
View(df)
View(df)
df <- as.matrix(df)
rm(df)
df <- cbind(missing.file.2, missing.file.3)
df <- cbind(missing.file.2, missing.file.3)
rm(df, missing.file.4)
rm(x)
df <- do.call(rbind.data.frame, Map('c', missing.file.2, missing.file.3))
colnames(df) <- c("date", "hour")
write.csv(df, "missing_date.csv")
View(df)
#install.packages("xts")
library(xts)
# generate a set of times from 2010-06-30 onwards at 20 minute intervals
tms <- as.POSIXct(seq(0,3600*24*30,by=60*20),origin="2010-06-30")
n   <- length(tms)
# generate volumes for those intervals, random 0 -- 100, turn into xts object
xts.ts <- xts(sample.int(100,n,replace=T),tms)
colnames(xts.ts)<-'Volume'
head(xts.ts)
apply.daily(xts.ts,sum)
apply.weekly(xts.ts,sum)
View(xts.ts)
install.packages("diamond")
library(ggplot2)
data(diamonds)
hist(diamonds$price)
summary(diamonds$price)
length(diamonds[, diamonds$price < 500])
diamonds[, diamonds$price < 500]
diamonds[diamonds$price < 500, ]
nrow(diamonds[diamonds$price < 500, ])
nrow(diamonds[diamonds$price < 250, ])
nrow(diamonds[diamonds$price > 15000, ])
nrow(diamonds[diamonds$price >= 15000, ])
hist(diamonds$price, breaks = 30)
hist(diamonds$price, breaks = 40)
hist(diamonds$price, breaks = 50)
hist(diamonds$price, breaks = 20)
hist(diamonds$price, breaks = 25)
hist(diamonds$price, breaks = 30)
ggplot(diamonds, aes(x=price))+
geom_histogram()+
facet_grid(~cut)+
theme_bw()
ddply(diamonds, ~cut, summarise, mean=mean(price), sd=sd(price))
library(dplyr)
ddply(diamonds, ~cut, summarise, mean=mean(price), sd=sd(price))
library(plyr)
ddply(diamonds, ~cut, summarise, mean=mean(price), sd=sd(price))
ddply(diamonds, ~cut, summarise, min=min(price), max=max(price))
ddply(diamonds, ~cut, summarise, min=min(price), max=max(price), median=median(price))
View(diamonds)
qplot(x = price, data = diamonds) + facet_wrap(~cut)
qplot(x = price, data = diamonds) + facet_wrap(~cut, scales = "free")
qplot(x = price/carat, data = diamonds) + facet_wrap(~cut, scales = "free")
qplot(x = price/carat, data = diamonds, log10 = "x") + facet_wrap(~cut, scales = "free")
qplot(x = price/carat, data = diamonds) + facet_wrap(~cut, scales = "free")
+ scale_x_log10()
qplot(x = price/carat, data = diamonds) + facet_wrap(~cut, scales = "free")
+ scale_x_continuous(formatter="log10")
qplot(x = price/carat, data = diamonds) + facet_wrap(~cut, scales = "free")
+ scale_x_continuous(trans = "log10")
ggplot(diamonds, aes(x=price/carat))+
scale_x_continuous(trans = "log10")
geom_histogram()+
facet_grid(~cut, scales = "free")+
theme_bw()
ggplot(diamonds, aes(x=price/carat))+
scale_x_continuous(trans = "log10")
geom_histogram()+
facet_grid(~cut)+
theme_bw()
ggplot(diamonds, aes(x=price/carat))+
geom_histogram()+
facet_grid(~cut)+
scale_x_continuous(trans = "log10")+
theme_bw()
ggplot(diamonds, aes(x=price/carat))+
geom_histogram()+
facet_grid(~cut, scales = "free")+
scale_x_continuous(trans = "log10")+
theme_bw()
View(diamonds)
ddply(diamonds, ~color, summarise,
quantile=matrix(quantile(price, probs=c(0.25,0.50,0.75)), ncol=3))
ggplot(diamonds, aes(x=price/carat))+
geom_boxplot()+
facet_grid(~cut, scales = "free")
theme_bw()
ggplot(diamonds, aes(x=price/carat))+
geom_boxplot()+
facet_grid(~cut, scales = "free")+
theme_bw()
ggplot(diamonds, aes(x=price/carat))+
geom_boxplot()+
facet_grid(~color, scales = "free")+
theme_bw()
ggplot(diamonds, aes(y=price/carat))+
geom_boxplot()+
facet_grid(~color, scales = "free")+
theme_bw()
ggplot(diamonds, aes(x=color, y=price/carat))+
geom_boxplot()+
theme_bw()
ggplot(diamonds, aes(carat)) +
geom_freqpoly()
ggplot(diamonds, aes(carat)) +
geom_freqpoly(stat = "bin")+
stat_bin(binwidth = 50)
ggplot(diamonds, aes(carat)) +
geom_freqpoly(stat = "bin")+
stat_bin(bins = 50)
ggplot(diamonds, aes(carat)) +
geom_freqpoly(stat = "bin", bins = 50)
ggplot(diamonds, aes(carat)) +
geom_freqpoly(stat = "bin", bins = 50)+
geom_point()+
stat_bin(bins = 50)
ggplot(diamonds, aes(y=carat)) +
geom_freqpoly(stat = "bin", bins = 50)+
geom_point()+
stat_bin(bins = 50)
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin", bins = 50)
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
bins = 50,
binwidth = 0.5)
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
bins = 50,
binwidth = 0.05)
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
bins = 50,
binwidth = 0.05) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.01),
bins = 50,
binwidth = 0.05) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
bins = 50,
binwidth = 0.05) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 8, by = 0.1),
bins = 50,
binwidth = 0.05) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
bins = 50,
binwidth = 0.01) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
ggplot(diamonds, aes(carat)) +
geom_histogram(stat = "bin",
breaks=seq(0, 5, by = 0.1),
binwidth = 0.01) +
scale_y_continuous(breaks = seq(0, 12000, by = 1000))
View(diamonds)
table(diamonds$carat)
install.packages("tidyr")
library(caret)
library(FSelector)
library(mlbench)
library(NoiseFiltersR)
library(rattle)
library(rpart)
library(rpart.plot)
setwd("C:/Project/ChurnPrediction/development/tuscany/")
set.seed(1234)
# read data
data <- read.csv("./df_user_level_label.csv")
# data <- data[data$churn != 1, ]
data$perc_coverage_issue <- data$count_visit_coverage_issue/data$visit_count
data$perc_scrubbing_issue <- data$count_visit_scrubbing_issue/data$visit_count
data$perc_pressure_issue <- data$count_visit_pressure_issue/data$visit_count
data$avg_mode_clean <- data$count_mode_clean/data$visit_count
data$avg_mode_white <- data$count_mode_white/data$visit_count
data$avg_mode_deep <- data$count_mode_deep/data$visit_count
data$avg_mode_gum <- data$count_mode_gum/data$visit_count
data$avg_mode_tongue <- data$count_mode_tongue/data$visit_count
data$avg_online_sessions <- data$count_online_sessions/data$visit_count
data$avg_offline_sessions <- data$count_offline_sessions/data$visit_count
data$avg_visit_morning <- data$count_visit_morning/data$visit_count
data$avg_visit_evening <- data$count_visit_evening/data$visit_count
data$churn[which(data$churn == 2)] <- 1
# data$avg_coverage_issue <- data$avg_coverage_issue - 1
# data$avg_scrubbing_issue <- data$avg_scrubbing_issue - 1
# data$avg_pressure_issue <- data$avg_pressure_issue - 1
# drop several columns
drops <- c('visitor_id', 'count_mode_gum', 'count_mode_tongue', 'prod_registered')
data <- data[ , !(names(data) %in% drops)]
data[is.na(data)] <- 0
# summarize the correlation matrix
cor_matrix <- cor(data[,1:32])
# find attributes that are highly correlated
cor_high <- findCorrelation(cor_matrix, cutoff=0.75, names = TRUE)
# remove highly correlated columns: number of visits
data <- data[ , !(names(data) %in% cor_high)]
# ----------------------- split to training and testing ------------------------
data$churn <- as.factor(data$churn)
# split data to training and test
nobs <- nrow(data)
train <- sample(nrow(data), 0.8 * nobs)
data_train <- data[train, ]
test <- setdiff(seq_len(nrow(data)), train)
data_test <- data[test, ]
data_train_dnoise = C45robustFilter(churn~., data_train)$cleanData
dt_model <- rpart(churn ~
avg_coverage_issue+
avg_scrubbing_issue+
avg_pressure_issue+
avg_mode_clean+
avg_mode_white+
avg_mode_deep+
avg_mode_gum+
avg_mode_tongue+
avg_online_sessions+
avg_offline_sessions+
avg_visit_morning+
avg_visit_evening,
data = data_train_dnoise,
method = "class",
parms = list(split="gini"),
control = rpart.control(maxdepth = 7, cp = 0.01, xval = 10))
pred = predict(dt_model, newdata = data_test, type = "class")
fancyRpartPlot(dt_model)
confusionMatrix(pred, data_test$churn, positive = "1")
pf <- read.delim('./pseudo_facebook.tsv')
setwd("C:/Project/Udacity/4 ExploreAndSummarizeData")
pf <- read.delim('./pseudo_facebook.tsv')
suppressMessages(library(dplyr))
pf <- read.delim('./pseudo_facebook.tsv')
suppressMessages(library(dplyr))
View(pf)
pf %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = size(friend_count)) %>%
pf.fc_by_age_gender
pf %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = length(friend_count)) %>%
pf.fc_by_age_gender
pf %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = length(friend_count)) %>%
k
pf.fc_by_age_gender <- pf %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = length(friend_count))
View(pf.fc_by_age_gender)
View(pf.fc_by_age_gender)
grouped <- group_by(mtcars, cyl)
groups(grouped)
groups(ungroup(grouped))
groups(pf.fc_by_age_gender)
pf.fc_by_age_gender <- pf %>%
filter(!is.na(gender)) %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = n()) %>%
ungroup() %>%
arrange(age)
rm(grouped)
View(pf.fc_by_age_gender)
View(pf.fc_by_age_gender)
library(ggplot2)
ggplot(data = pf.fc_by_age_gender,
aes(x = age, y = median_friend_count, group = gender)) +
geom_line()
library(ggplot2)
ggplot(data = pf.fc_by_age_gender,
aes(x = age, y = median_friend_count, group = gender, color = gender)) +
geom_line()
library(caret)
library(FSelector)
library(mlbench)
library(NoiseFiltersR)
library(rattle)
library(rpart)
library(rpart.plot)
setwd("C:/Project/ChurnPrediction/development/tuscany/")
set.seed(1234)
# read data
data <- read.csv("./df_user_level_label.csv")
pf <- read.delim('./pseudo_facebook.tsv')
setwd("C:/Project/Udacity/4 ExploreAndSummarizeData")
pf <- read.delim('./pseudo_facebook.tsv')
pf.fc_by_age_gender <- pf %>%
filter(!is.na(gender)) %>%
group_by(age, gender) %>%
summarise (mean_friend_count = mean(friend_count),
median_friend_count = median(friend_count),
n = n()) %>%
ungroup() %>%
arrange(age)
pf.fc_by_age_gender.wide <- dcast(pf.fc_by_age_gender, age ~ gender,
value.var = 'median_friend')
library(reshape2)
pf.fc_by_age_gender.wide <- dcast(pf.fc_by_age_gender, age ~ gender,
value.var = 'median_friend')
View(pf.fc_by_age_gender)
pf.fc_by_age_gender.wide <- dcast(pf.fc_by_age_gender, age ~ gender,
value.var = 'median_friend_count')
View(pf.fc_by_age_gender.wide)
View(pf.fc_by_age_gender)
